{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7818efe4",
   "metadata": {},
   "source": [
    "# claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af29b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n",
      "File \u001b[0;32m/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('training_data/train/train.csv')\n",
    "df_transactions = pd.read_csv('training_data/train/transactions.csv')\n",
    "df_test = pd.read_csv('testing data/test_8gqdJqH.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Transactions shape: {df_transactions.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 6. DATA QUALITY CHECKS\n",
    "# ========================================\n",
    "\n",
    "def perform_data_quality_checks(df, name):\n",
    "    print(f\"\\n=== Data Quality Checks for {name} ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check for negative values in numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"Negative values in {col}: {negative_count}\")\n",
    "    \n",
    "    # Check date format consistency\n",
    "    date_cols = [col for col in df.columns if 'do' in col.lower()]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                pd.to_datetime(df[col])\n",
    "                print(f\"Date column {col}: Valid format\")\n",
    "            except:\n",
    "                print(f\"Date column {col}: Invalid format detected\")\n",
    "    \n",
    "    # Check for outliers using IQR method\n",
    "    for col in numeric_cols:\n",
    "        if col not in ['srcid', 'destid']:  # Skip ID columns\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "            print(f\"Outliers in {col}: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Perform quality checks\n",
    "perform_data_quality_checks(df_train, 'Train')\n",
    "perform_data_quality_checks(df_transactions, 'Transactions')\n",
    "perform_data_quality_checks(df_test, 'Test')\n",
    "\n",
    "# ========================================\n",
    "# 7. FEATURE ENGINEERING\n",
    "# ========================================\n",
    "\n",
    "def create_route_key(df):\n",
    "    \"\"\"Create route key from doj, srcid, destid\"\"\"\n",
    "    df['route_key'] = df['doj'].astype(str) + '_' + df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "    return df\n",
    "\n",
    "def extract_date_features(df, date_col):\n",
    "    \"\"\"Extract various date features\"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_month'] = df[date_col].dt.month\n",
    "    df[f'{date_col}_day'] = df[date_col].dt.day\n",
    "    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek\n",
    "    df[f'{date_col}_quarter'] = df[date_col].dt.quarter\n",
    "    df[f'{date_col}_is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)\n",
    "    df[f'{date_col}_is_month_start'] = df[date_col].dt.is_month_start.astype(int)\n",
    "    df[f'{date_col}_is_month_end'] = df[date_col].dt.is_month_end.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, group_cols, target_col, lags=[1, 2, 3, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sort_values(group_cols + ['doj'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby(group_cols)[target_col].shift(lag)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def create_rolling_features(df, group_cols, target_col, windows=[3, 7, 14, 30]):\n",
    "    \"\"\"Create rolling window features\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sort_values(group_cols + ['doj'])\n",
    "    \n",
    "    for window in windows:\n",
    "        df_copy[f'{target_col}_rolling_mean_{window}'] = df_copy.groupby(group_cols)[target_col].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df_copy[f'{target_col}_rolling_std_{window}'] = df_copy.groupby(group_cols)[target_col].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "        df_copy[f'{target_col}_rolling_max_{window}'] = df_copy.groupby(group_cols)[target_col].rolling(window=window, min_periods=1).max().reset_index(0, drop=True)\n",
    "        df_copy[f'{target_col}_rolling_min_{window}'] = df_copy.groupby(group_cols)[target_col].rolling(window=window, min_periods=1).min().reset_index(0, drop=True)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def create_aggregated_features(transactions_df):\n",
    "    \"\"\"Create aggregated features from transactions data\"\"\"\n",
    "    \n",
    "    # Convert dates\n",
    "    transactions_df['doj'] = pd.to_datetime(transactions_df['doj'])\n",
    "    transactions_df['doi'] = pd.to_datetime(transactions_df['doi'])\n",
    "    \n",
    "    # Create route combinations\n",
    "    transactions_df['route'] = transactions_df['srcid'].astype(str) + '_' + transactions_df['destid'].astype(str)\n",
    "    \n",
    "    # Aggregate features at route-date level (15 days before doj)\n",
    "    # For prediction, we need data available 15 days before journey\n",
    "    transactions_df['prediction_date'] = transactions_df['doj'] - pd.Timedelta(days=15)\n",
    "    \n",
    "    # Filter transactions that would be available 15 days before journey\n",
    "    available_transactions = transactions_df[transactions_df['doi'] <= transactions_df['prediction_date']].copy()\n",
    "    \n",
    "    # Group by route and doj to create features\n",
    "    agg_features = available_transactions.groupby(['doj', 'srcid', 'destid']).agg({\n",
    "        'cumsum_seatcount': ['max', 'mean', 'std', 'count'],\n",
    "        'cumsum_searchcount': ['max', 'mean', 'std', 'count'],\n",
    "        'dbd': ['min', 'max', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    agg_features.columns = ['doj', 'srcid', 'destid'] + [f'{col[0]}_{col[1]}' for col in agg_features.columns[3:]]\n",
    "    \n",
    "    # Fill NaN values with 0 for std (when count is 1)\n",
    "    agg_features = agg_features.fillna(0)\n",
    "    \n",
    "    # Add route-level historical features\n",
    "    route_history = available_transactions.groupby(['srcid', 'destid']).agg({\n",
    "        'cumsum_seatcount': ['mean', 'std', 'max'],\n",
    "        'cumsum_searchcount': ['mean', 'std', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    route_history.columns = ['srcid', 'destid'] + [f'route_{col[0]}_{col[1]}' for col in route_history.columns[2:]]\n",
    "    route_history = route_history.fillna(0)\n",
    "    \n",
    "    # Merge route history\n",
    "    agg_features = agg_features.merge(route_history, on=['srcid', 'destid'], how='left')\n",
    "    \n",
    "    # Add region and tier information\n",
    "    region_tier_info = transactions_df[['srcid', 'destid', 'srcid_region', 'destid_region', \n",
    "                                       'srcid_tier', 'destid_tier']].drop_duplicates()\n",
    "    \n",
    "    agg_features = agg_features.merge(region_tier_info, on=['srcid', 'destid'], how='left')\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "\n",
    "# Create route keys\n",
    "df_train = create_route_key(df_train)\n",
    "df_test = create_route_key(df_test)\n",
    "\n",
    "# Extract date features\n",
    "df_train = extract_date_features(df_train, 'doj')\n",
    "df_test = extract_date_features(df_test, 'doj')\n",
    "\n",
    "# Create aggregated features from transactions\n",
    "agg_features = create_aggregated_features(df_transactions)\n",
    "\n",
    "# Merge with training data\n",
    "df_train_features = df_train.merge(agg_features, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "# Merge with test data\n",
    "df_test_features = df_test.merge(agg_features, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "# Handle missing values (routes not seen in transactions)\n",
    "numeric_cols = df_train_features.select_dtypes(include=[np.number]).columns\n",
    "df_train_features[numeric_cols] = df_train_features[numeric_cols].fillna(0)\n",
    "df_test_features[numeric_cols] = df_test_features[numeric_cols].fillna(0)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_train_features.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle unseen categories\n",
    "        combined_values = pd.concat([df_train_features[col].fillna('Unknown'), \n",
    "                                   df_test_features[col].fillna('Unknown')])\n",
    "        le.fit(combined_values)\n",
    "        \n",
    "        df_train_features[col] = le.transform(df_train_features[col].fillna('Unknown'))\n",
    "        df_test_features[col] = le.transform(df_test_features[col].fillna('Unknown'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Training features shape: {df_train_features.shape}\")\n",
    "print(f\"Test features shape: {df_test_features.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 8. MODEL TRAINING AND EVALUATION\n",
    "# ========================================\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in df_train_features.columns \n",
    "                if col not in ['route_key', 'doj', 'final_seatcount']]\n",
    "\n",
    "X = df_train_features[feature_cols]\n",
    "y = df_train_features['final_seatcount']\n",
    "X_test = df_test_features[feature_cols]\n",
    "\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}): {feature_cols[:10]}...\")  # Show first 10\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Time series split for validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# ========================================\n",
    "# 9. MULTIPLE MODELS WITH HYPERPARAMETER TUNING\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Evaluate model and return RMSE\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f\"{model_name} RMSE: {rmse:.2f}\")\n",
    "    return rmse, model\n",
    "\n",
    "# Store results\n",
    "model_results = {}\n",
    "\n",
    "print(\"\\n=== Model Training and Evaluation ===\")\n",
    "\n",
    "# 1. Random Forest with GridSearch\n",
    "print(\"\\n1. Random Forest...\")\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Use a subset for faster grid search\n",
    "sample_size = min(10000, len(X))\n",
    "sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "rf_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(f\"Best RF params: {rf_grid.best_params_}\")\n",
    "\n",
    "# Evaluate on time series split\n",
    "rf_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_rf, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"RF\")\n",
    "    rf_scores.append(rmse)\n",
    "\n",
    "model_results['Random Forest'] = {\n",
    "    'mean_rmse': np.mean(rf_scores),\n",
    "    'std_rmse': np.std(rf_scores),\n",
    "    'model': best_rf\n",
    "}\n",
    "\n",
    "# 2. XGBoost\n",
    "print(\"\\n2. XGBoost...\")\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "xgb_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "print(f\"Best XGB params: {xgb_grid.best_params_}\")\n",
    "\n",
    "xgb_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_xgb, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"XGB\")\n",
    "    xgb_scores.append(rmse)\n",
    "\n",
    "model_results['XGBoost'] = {\n",
    "    'mean_rmse': np.mean(xgb_scores),\n",
    "    'std_rmse': np.std(xgb_scores),\n",
    "    'model': best_xgb\n",
    "}\n",
    "\n",
    "# 3. LightGBM\n",
    "print(\"\\n3. LightGBM...\")\n",
    "lgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'subsample': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1)\n",
    "lgb_grid = GridSearchCV(lgb, lgb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "lgb_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_lgb = lgb_grid.best_estimator_\n",
    "print(f\"Best LGB params: {lgb_grid.best_params_}\")\n",
    "\n",
    "lgb_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_lgb, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"LGB\")\n",
    "    lgb_scores.append(rmse)\n",
    "\n",
    "model_results['LightGBM'] = {\n",
    "    'mean_rmse': np.mean(lgb_scores),\n",
    "    'std_rmse': np.std(lgb_scores),\n",
    "    'model': best_lgb\n",
    "}\n",
    "\n",
    "# 4. Gradient Boosting\n",
    "print(\"\\n4. Gradient Boosting...\")\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [6, 8],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gb_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_gb = gb_grid.best_estimator_\n",
    "print(f\"Best GB params: {gb_grid.best_params_}\")\n",
    "\n",
    "gb_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_gb, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"GB\")\n",
    "    gb_scores.append(rmse)\n",
    "\n",
    "model_results['Gradient Boosting'] = {\n",
    "    'mean_rmse': np.mean(gb_scores),\n",
    "    'std_rmse': np.std(gb_scores),\n",
    "    'model': best_gb\n",
    "}\n",
    "\n",
    "# 5. Ridge Regression\n",
    "print(\"\\n5. Ridge Regression...\")\n",
    "ridge_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_scaled[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "print(f\"Best Ridge params: {ridge_grid.best_params_}\")\n",
    "\n",
    "ridge_scores = []\n",
    "for train_idx, val_idx in tscv.split(X_scaled):\n",
    "    X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_ridge, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"Ridge\")\n",
    "    ridge_scores.append(rmse)\n",
    "\n",
    "model_results['Ridge'] = {\n",
    "    'mean_rmse': np.mean(ridge_scores),\n",
    "    'std_rmse': np.std(ridge_scores),\n",
    "    'model': best_ridge\n",
    "}\n",
    "\n",
    "# Print model comparison\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"{model_name}: {results['mean_rmse']:.2f} ± {results['std_rmse']:.2f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['mean_rmse'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "print(f\"\\nBest Model: {best_model_name} (RMSE: {model_results[best_model_name]['mean_rmse']:.2f})\")\n",
    "\n",
    "# ========================================\n",
    "# 10. ENSEMBLE MODEL\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n=== Creating Ensemble Model ===\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_models = [model_results[name]['model'] for name in ['Random Forest', 'XGBoost', 'LightGBM']]\n",
    "ensemble_weights = [1/len(ensemble_models)] * len(ensemble_models)  # Equal weights\n",
    "\n",
    "def ensemble_predict(models, X_data, weights=None, use_scaled=False):\n",
    "    if weights is None:\n",
    "        weights = [1/len(models)] * len(models)\n",
    "    \n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        if isinstance(model, (Ridge, Lasso)) and use_scaled:\n",
    "            pred = model.predict(X_data)\n",
    "        else:\n",
    "            pred = model.predict(X_data if not use_scaled else X_data)\n",
    "        predictions.append(pred * weights[i])\n",
    "    \n",
    "    return np.sum(predictions, axis=0)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train models on fold\n",
    "    fold_models = []\n",
    "    for model in ensemble_models:\n",
    "        model_copy = type(model)(**model.get_params())\n",
    "        model_copy.fit(X_train_fold, y_train_fold)\n",
    "        fold_models.append(model_copy)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = ensemble_predict(fold_models, X_val_fold)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "    ensemble_scores.append(rmse)\n",
    "    print(f\"Ensemble RMSE: {rmse:.2f}\")\n",
    "\n",
    "ensemble_mean_rmse = np.mean(ensemble_scores)\n",
    "print(f\"Ensemble Mean RMSE: {ensemble_mean_rmse:.2f} ± {np.std(ensemble_scores):.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 11. FINAL PREDICTIONS AND SUBMISSION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n=== Final Training and Predictions ===\")\n",
    "\n",
    "# Train final models on full dataset\n",
    "final_models = []\n",
    "for model in ensemble_models:\n",
    "    final_model = type(model)(**model.get_params())\n",
    "    final_model.fit(X, y)\n",
    "    final_models.append(final_model)\n",
    "\n",
    "# Make final predictions\n",
    "if ensemble_mean_rmse < model_results[best_model_name]['mean_rmse']:\n",
    "    print(\"Using ensemble model for final predictions\")\n",
    "    final_predictions = ensemble_predict(final_models, X_test)\n",
    "else:\n",
    "    print(f\"Using {best_model_name} for final predictions\")\n",
    "    best_model.fit(X, y)\n",
    "    if isinstance(best_model, (Ridge, Lasso)):\n",
    "        final_predictions = best_model.predict(X_test_scaled)\n",
    "    else:\n",
    "        final_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "final_predictions = np.maximum(final_predictions, 0)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'route_key': df_test['route_key'],\n",
    "    'final_seatcount': final_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission_file.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission file created with {len(submission)} predictions\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"Mean: {final_predictions.mean():.2f}\")\n",
    "print(f\"Std: {final_predictions.std():.2f}\")\n",
    "print(f\"Min: {final_predictions.min():.2f}\")\n",
    "print(f\"Max: {final_predictions.max():.2f}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n=== Feature Importance (from best tree-based model) ===\")\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting']:\n",
    "    importance = best_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(15))\n",
    "\n",
    "print(\"\\n=== Solution Complete ===\")\n",
    "print(\"Files created:\")\n",
    "print(\"- submission_file.csv: Final predictions for submission\")\n",
    "print(f\"- Best model: {best_model_name}\")\n",
    "print(f\"- Expected RMSE: {model_results[best_model_name]['mean_rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435989cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train shape: (67200, 4)\n",
      "Transactions shape: (2266100, 11)\n",
      "Test shape: (5900, 4)\n",
      "\n",
      "=== Data Quality Checks for Train ===\n",
      "Missing values:\n",
      "doj                0\n",
      "srcid              0\n",
      "destid             0\n",
      "final_seatcount    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Data types:\n",
      "doj                 object\n",
      "srcid                int64\n",
      "destid               int64\n",
      "final_seatcount    float64\n",
      "dtype: object\n",
      "Date column doj: Valid format\n",
      "Outliers in final_seatcount: 4141 (6.16%)\n",
      "\n",
      "=== Data Quality Checks for Transactions ===\n",
      "Missing values:\n",
      "doj                   0\n",
      "doi                   0\n",
      "srcid                 0\n",
      "destid                0\n",
      "srcid_region          0\n",
      "destid_region         0\n",
      "srcid_tier            0\n",
      "destid_tier           0\n",
      "cumsum_seatcount      0\n",
      "cumsum_searchcount    0\n",
      "dbd                   0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Data types:\n",
      "doj                    object\n",
      "doi                    object\n",
      "srcid                   int64\n",
      "destid                  int64\n",
      "srcid_region           object\n",
      "destid_region          object\n",
      "srcid_tier             object\n",
      "destid_tier            object\n",
      "cumsum_seatcount      float64\n",
      "cumsum_searchcount    float64\n",
      "dbd                     int64\n",
      "dtype: object\n",
      "Date column doj: Valid format\n",
      "Date column doi: Valid format\n",
      "Outliers in cumsum_seatcount: 333035 (14.70%)\n",
      "Outliers in cumsum_searchcount: 309897 (13.68%)\n",
      "Outliers in dbd: 0 (0.00%)\n",
      "\n",
      "=== Data Quality Checks for Test ===\n",
      "Missing values:\n",
      "route_key    0\n",
      "doj          0\n",
      "srcid        0\n",
      "destid       0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Data types:\n",
      "route_key    object\n",
      "doj          object\n",
      "srcid         int64\n",
      "destid        int64\n",
      "dtype: object\n",
      "Date column doj: Valid format\n",
      "\n",
      "Creating features...\n",
      "Training features shape: (67200, 53)\n",
      "Test features shape: (5900, 52)\n",
      "\n",
      "Feature columns (50): ['srcid', 'destid', 'doj_year', 'doj_month', 'doj_day', 'doj_dayofweek', 'doj_quarter', 'doj_is_weekend', 'doj_is_month_start', 'doj_is_month_end']...\n",
      "\n",
      "=== Model Training and Evaluation ===\n",
      "\n",
      "1. Random Forest...\n",
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best RF params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "RF RMSE: 595.74\n",
      "RF RMSE: 670.40\n",
      "RF RMSE: 493.48\n",
      "RF RMSE: 451.21\n",
      "RF RMSE: 600.22\n",
      "\n",
      "2. Extra Trees...\n",
      "Best ET params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "ET RMSE: 600.90\n",
      "ET RMSE: 644.72\n",
      "ET RMSE: 490.42\n",
      "ET RMSE: 442.05\n",
      "ET RMSE: 582.47\n",
      "\n",
      "3. Gradient Boosting...\n",
      "Best GB params: {'learning_rate': 0.05, 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 300, 'subsample': 1.0}\n",
      "GB RMSE: 574.21\n",
      "GB RMSE: 648.19\n",
      "GB RMSE: 462.11\n",
      "GB RMSE: 403.62\n",
      "GB RMSE: 600.26\n",
      "\n",
      "4. Ridge Regression...\n",
      "Best Ridge params: {'alpha': 1.0, 'solver': 'svd'}\n",
      "Ridge RMSE: 1727.78\n",
      "Ridge RMSE: 1508.82\n",
      "Ridge RMSE: 829.77\n",
      "Ridge RMSE: 756.00\n",
      "Ridge RMSE: 832.23\n",
      "\n",
      "5. ElasticNet...\n",
      "Best ElasticNet params: {'alpha': 0.1, 'l1_ratio': 0.9}\n",
      "ElasticNet RMSE: 763.99\n",
      "ElasticNet RMSE: 1300.59\n",
      "ElasticNet RMSE: 788.30\n",
      "ElasticNet RMSE: 783.30\n",
      "ElasticNet RMSE: 865.98\n",
      "\n",
      "=== Model Comparison ===\n",
      "Random Forest: 562.21 ± 79.14\n",
      "Extra Trees: 552.11 ± 74.56\n",
      "Gradient Boosting: 537.68 ± 90.72\n",
      "Ridge: 1130.92 ± 404.85\n",
      "ElasticNet: 900.43 ± 203.09\n",
      "\n",
      "Best Model: Gradient Boosting (RMSE: 537.68)\n",
      "\n",
      "=== Creating Ensemble Model ===\n",
      "Top 3 models for ensemble:\n",
      "- Gradient Boosting: 537.68\n",
      "- Extra Trees: 552.11\n",
      "- Random Forest: 562.21\n",
      "Ensemble RMSE: 583.79\n",
      "Ensemble RMSE: 642.00\n",
      "Ensemble RMSE: 471.97\n",
      "Ensemble RMSE: 420.18\n",
      "Ensemble RMSE: 584.97\n",
      "Ensemble Mean RMSE: 540.58 ± 81.64\n",
      "\n",
      "=== Final Training and Predictions ===\n",
      "Using Gradient Boosting for final predictions\n",
      "\n",
      "Submission file created with 5900 predictions\n",
      "Prediction statistics:\n",
      "Mean: 2162.13\n",
      "Std: 1178.96\n",
      "Min: 25.26\n",
      "Max: 10895.42\n",
      "\n",
      "First 10 predictions:\n",
      "          route_key  final_seatcount\n",
      "0  2025-02-11_46_45      3739.963130\n",
      "1  2025-01-20_17_23      1554.948866\n",
      "2   2025-01-08_2_14      1090.680316\n",
      "3   2025-01-08_8_47      1021.806248\n",
      "4   2025-01-08_9_46      3365.741193\n",
      "5   2025-01-21_45_5      1056.328638\n",
      "6   2025-02-26_47_8      1427.995021\n",
      "7   2025-01-03_2_19      2157.571029\n",
      "8   2025-02-11_2_30      1615.516800\n",
      "9   2025-01-25_5_45      1707.297766\n",
      "\n",
      "=== Feature Importance (from best tree-based model) ===\n",
      "                          feature  importance\n",
      "21         cumsum_searchcount_std    0.131538\n",
      "23         cumsum_searchcount_sum    0.086158\n",
      "19         cumsum_searchcount_max    0.081773\n",
      "20        cumsum_searchcount_mean    0.072183\n",
      "14           cumsum_seatcount_max    0.044040\n",
      "1                          destid    0.043540\n",
      "35   route_cumsum_searchcount_min    0.042850\n",
      "32  route_cumsum_searchcount_mean    0.033948\n",
      "16           cumsum_seatcount_std    0.033832\n",
      "29     route_cumsum_seatcount_std    0.029551\n",
      "28    route_cumsum_seatcount_mean    0.026110\n",
      "41       src_cumsum_seatcount_std    0.025872\n",
      "34   route_cumsum_searchcount_max    0.024911\n",
      "30     route_cumsum_seatcount_max    0.023024\n",
      "33   route_cumsum_searchcount_std    0.022581\n",
      "\n",
      "=== Solution Complete ===\n",
      "Files created:\n",
      "- submission_file.csv: Final predictions for submission\n",
      "- Best model: Gradient Boosting\n",
      "- Expected RMSE: 537.68\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model comparison plot saved as 'model_comparison.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('training_data/train/train.csv')\n",
    "df_transactions = pd.read_csv('training_data/train/transactions.csv')\n",
    "df_test = pd.read_csv('testing data/test_8gqdJqH.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Transactions shape: {df_transactions.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 6. DATA QUALITY CHECKS\n",
    "# ========================================\n",
    "\n",
    "def perform_data_quality_checks(df, name):\n",
    "    print(f\"\\n=== Data Quality Checks for {name} ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check for negative values in numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"Negative values in {col}: {negative_count}\")\n",
    "    \n",
    "    # Check date format consistency\n",
    "    date_cols = [col for col in df.columns if 'do' in col.lower()]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                pd.to_datetime(df[col])\n",
    "                print(f\"Date column {col}: Valid format\")\n",
    "            except:\n",
    "                print(f\"Date column {col}: Invalid format detected\")\n",
    "    \n",
    "    # Check for outliers using IQR method\n",
    "    for col in numeric_cols:\n",
    "        if col not in ['srcid', 'destid']:  # Skip ID columns\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "            print(f\"Outliers in {col}: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Perform quality checks\n",
    "perform_data_quality_checks(df_train, 'Train')\n",
    "perform_data_quality_checks(df_transactions, 'Transactions')\n",
    "perform_data_quality_checks(df_test, 'Test')\n",
    "\n",
    "# ========================================\n",
    "# 7. FEATURE ENGINEERING\n",
    "# ========================================\n",
    "\n",
    "def create_route_key(df):\n",
    "    \"\"\"Create route key from doj, srcid, destid\"\"\"\n",
    "    df['route_key'] = df['doj'].astype(str) + '_' + df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "    return df\n",
    "\n",
    "def extract_date_features(df, date_col):\n",
    "    \"\"\"Extract various date features\"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_month'] = df[date_col].dt.month\n",
    "    df[f'{date_col}_day'] = df[date_col].dt.day\n",
    "    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek\n",
    "    df[f'{date_col}_quarter'] = df[date_col].dt.quarter\n",
    "    df[f'{date_col}_is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)\n",
    "    df[f'{date_col}_is_month_start'] = df[date_col].dt.is_month_start.astype(int)\n",
    "    df[f'{date_col}_is_month_end'] = df[date_col].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Add cyclical features for better temporal understanding\n",
    "    df[f'{date_col}_month_sin'] = np.sin(2 * np.pi * df[f'{date_col}_month'] / 12)\n",
    "    df[f'{date_col}_month_cos'] = np.cos(2 * np.pi * df[f'{date_col}_month'] / 12)\n",
    "    df[f'{date_col}_dayofweek_sin'] = np.sin(2 * np.pi * df[f'{date_col}_dayofweek'] / 7)\n",
    "    df[f'{date_col}_dayofweek_cos'] = np.cos(2 * np.pi * df[f'{date_col}_dayofweek'] / 7)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(transactions_df):\n",
    "    \"\"\"Create aggregated features from transactions data\"\"\"\n",
    "    \n",
    "    # Convert dates\n",
    "    transactions_df['doj'] = pd.to_datetime(transactions_df['doj'])\n",
    "    transactions_df['doi'] = pd.to_datetime(transactions_df['doi'])\n",
    "    \n",
    "    # Create route combinations\n",
    "    transactions_df['route'] = transactions_df['srcid'].astype(str) + '_' + transactions_df['destid'].astype(str)\n",
    "    \n",
    "    # Aggregate features at route-date level (15 days before doj)\n",
    "    # For prediction, we need data available 15 days before journey\n",
    "    transactions_df['prediction_date'] = transactions_df['doj'] - pd.Timedelta(days=15)\n",
    "    \n",
    "    # Filter transactions that would be available 15 days before journey\n",
    "    available_transactions = transactions_df[transactions_df['doi'] <= transactions_df['prediction_date']].copy()\n",
    "    \n",
    "    # Group by route and doj to create features\n",
    "    agg_features = available_transactions.groupby(['doj', 'srcid', 'destid']).agg({\n",
    "        'cumsum_seatcount': ['max', 'mean', 'std', 'count', 'sum'],\n",
    "        'cumsum_searchcount': ['max', 'mean', 'std', 'count', 'sum'],\n",
    "        'dbd': ['min', 'max', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    agg_features.columns = ['doj', 'srcid', 'destid'] + [f'{col[0]}_{col[1]}' for col in agg_features.columns[3:]]\n",
    "    \n",
    "    # Fill NaN values with 0 for std (when count is 1)\n",
    "    agg_features = agg_features.fillna(0)\n",
    "    \n",
    "    # Add route-level historical features\n",
    "    route_history = available_transactions.groupby(['srcid', 'destid']).agg({\n",
    "        'cumsum_seatcount': ['mean', 'std', 'max', 'min'],\n",
    "        'cumsum_searchcount': ['mean', 'std', 'max', 'min']\n",
    "    }).reset_index()\n",
    "    \n",
    "    route_history.columns = ['srcid', 'destid'] + [f'route_{col[0]}_{col[1]}' for col in route_history.columns[2:]]\n",
    "    route_history = route_history.fillna(0)\n",
    "    \n",
    "    # Merge route history\n",
    "    agg_features = agg_features.merge(route_history, on=['srcid', 'destid'], how='left')\n",
    "    \n",
    "    # Add region and tier information\n",
    "    region_tier_info = transactions_df[['srcid', 'destid', 'srcid_region', 'destid_region', \n",
    "                                       'srcid_tier', 'destid_tier']].drop_duplicates()\n",
    "    \n",
    "    agg_features = agg_features.merge(region_tier_info, on=['srcid', 'destid'], how='left')\n",
    "    \n",
    "    # Add source and destination specific features\n",
    "    src_features = available_transactions.groupby(['srcid']).agg({\n",
    "        'cumsum_seatcount': ['mean', 'std'],\n",
    "        'cumsum_searchcount': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    src_features.columns = ['srcid'] + [f'src_{col[0]}_{col[1]}' for col in src_features.columns[1:]]\n",
    "    src_features = src_features.fillna(0)\n",
    "    \n",
    "    dest_features = available_transactions.groupby(['destid']).agg({\n",
    "        'cumsum_seatcount': ['mean', 'std'],\n",
    "        'cumsum_searchcount': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    dest_features.columns = ['destid'] + [f'dest_{col[0]}_{col[1]}' for col in dest_features.columns[1:]]\n",
    "    dest_features = dest_features.fillna(0)\n",
    "    \n",
    "    # Merge source and destination features\n",
    "    agg_features = agg_features.merge(src_features, on=['srcid'], how='left')\n",
    "    agg_features = agg_features.merge(dest_features, on=['destid'], how='left')\n",
    "    \n",
    "    # Create ratio features\n",
    "    agg_features['search_to_seat_ratio'] = agg_features['cumsum_searchcount_sum'] / (agg_features['cumsum_seatcount_sum'] + 1)\n",
    "    agg_features['booking_efficiency'] = agg_features['cumsum_seatcount_sum'] / (agg_features['cumsum_searchcount_sum'] + 1)\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "\n",
    "# Create route keys\n",
    "df_train = create_route_key(df_train)\n",
    "df_test = create_route_key(df_test)\n",
    "\n",
    "# Extract date features\n",
    "df_train = extract_date_features(df_train, 'doj')\n",
    "df_test = extract_date_features(df_test, 'doj')\n",
    "\n",
    "# Create aggregated features from transactions\n",
    "agg_features = create_aggregated_features(df_transactions)\n",
    "\n",
    "# Merge with training data\n",
    "df_train_features = df_train.merge(agg_features, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "# Merge with test data\n",
    "df_test_features = df_test.merge(agg_features, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "# Handle missing values (routes not seen in transactions)\n",
    "# FIXED: Get numeric columns separately for train and test data\n",
    "train_numeric_cols = df_train_features.select_dtypes(include=[np.number]).columns\n",
    "test_numeric_cols = df_test_features.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_train_features[train_numeric_cols] = df_train_features[train_numeric_cols].fillna(0)\n",
    "df_test_features[test_numeric_cols] = df_test_features[test_numeric_cols].fillna(0)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_train_features.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle unseen categories\n",
    "        combined_values = pd.concat([df_train_features[col].fillna('Unknown'), \n",
    "                                   df_test_features[col].fillna('Unknown')])\n",
    "        le.fit(combined_values)\n",
    "        \n",
    "        df_train_features[col] = le.transform(df_train_features[col].fillna('Unknown'))\n",
    "        df_test_features[col] = le.transform(df_test_features[col].fillna('Unknown'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Training features shape: {df_train_features.shape}\")\n",
    "print(f\"Test features shape: {df_test_features.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 8. MODEL TRAINING AND EVALUATION\n",
    "# ========================================\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in df_train_features.columns \n",
    "                if col not in ['route_key', 'doj', 'final_seatcount']]\n",
    "\n",
    "X = df_train_features[feature_cols]\n",
    "y = df_train_features['final_seatcount']\n",
    "X_test = df_test_features[feature_cols]\n",
    "\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}): {feature_cols[:10]}...\")  # Show first 10\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Time series split for validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# ========================================\n",
    "# 9. MULTIPLE MODELS WITH HYPERPARAMETER TUNING\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Evaluate model and return RMSE\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    print(f\"{model_name} RMSE: {rmse:.2f}\")\n",
    "    return rmse, model\n",
    "\n",
    "# Store results\n",
    "model_results = {}\n",
    "\n",
    "print(\"\\n=== Model Training and Evaluation ===\")\n",
    "\n",
    "# 1. Random Forest with GridSearch\n",
    "print(\"\\n1. Random Forest...\")\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Use a subset for faster grid search\n",
    "sample_size = min(15000, len(X))\n",
    "sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "rf_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(f\"Best RF params: {rf_grid.best_params_}\")\n",
    "\n",
    "# Evaluate on time series split\n",
    "rf_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_rf, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"RF\")\n",
    "    rf_scores.append(rmse)\n",
    "\n",
    "model_results['Random Forest'] = {\n",
    "    'mean_rmse': np.mean(rf_scores),\n",
    "    'std_rmse': np.std(rf_scores),\n",
    "    'model': best_rf\n",
    "}\n",
    "\n",
    "# 2. Extra Trees\n",
    "print(\"\\n2. Extra Trees...\")\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n",
    "et_grid = GridSearchCV(et, et_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "et_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_et = et_grid.best_estimator_\n",
    "print(f\"Best ET params: {et_grid.best_params_}\")\n",
    "\n",
    "et_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_et, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"ET\")\n",
    "    et_scores.append(rmse)\n",
    "\n",
    "model_results['Extra Trees'] = {\n",
    "    'mean_rmse': np.mean(et_scores),\n",
    "    'std_rmse': np.std(et_scores),\n",
    "    'model': best_et\n",
    "}\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "print(\"\\n3. Gradient Boosting...\")\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gb_grid.fit(X.iloc[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_gb = gb_grid.best_estimator_\n",
    "print(f\"Best GB params: {gb_grid.best_params_}\")\n",
    "\n",
    "gb_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_gb, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"GB\")\n",
    "    gb_scores.append(rmse)\n",
    "\n",
    "model_results['Gradient Boosting'] = {\n",
    "    'mean_rmse': np.mean(gb_scores),\n",
    "    'std_rmse': np.std(gb_scores),\n",
    "    'model': best_gb\n",
    "}\n",
    "\n",
    "# 4. Ridge Regression\n",
    "print(\"\\n4. Ridge Regression...\")\n",
    "ridge_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "    'solver': ['auto', 'svd', 'cholesky']\n",
    "}\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_scaled[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "print(f\"Best Ridge params: {ridge_grid.best_params_}\")\n",
    "\n",
    "ridge_scores = []\n",
    "for train_idx, val_idx in tscv.split(X_scaled):\n",
    "    X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_ridge, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"Ridge\")\n",
    "    ridge_scores.append(rmse)\n",
    "\n",
    "model_results['Ridge'] = {\n",
    "    'mean_rmse': np.mean(ridge_scores),\n",
    "    'std_rmse': np.std(ridge_scores),\n",
    "    'model': best_ridge,\n",
    "    'use_scaled': True\n",
    "}\n",
    "\n",
    "# 5. ElasticNet\n",
    "print(\"\\n5. ElasticNet...\")\n",
    "en_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "en = ElasticNet(random_state=42, max_iter=2000)\n",
    "en_grid = GridSearchCV(en, en_params, cv=5, scoring='neg_mean_squared_error')\n",
    "en_grid.fit(X_scaled[sample_idx], y.iloc[sample_idx])\n",
    "\n",
    "best_en = en_grid.best_estimator_\n",
    "print(f\"Best ElasticNet params: {en_grid.best_params_}\")\n",
    "\n",
    "en_scores = []\n",
    "for train_idx, val_idx in tscv.split(X_scaled):\n",
    "    X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    rmse, _ = evaluate_model(best_en, X_train_fold, X_val_fold, y_train_fold, y_val_fold, \"ElasticNet\")\n",
    "    en_scores.append(rmse)\n",
    "\n",
    "model_results['ElasticNet'] = {\n",
    "    'mean_rmse': np.mean(en_scores),\n",
    "    'std_rmse': np.std(en_scores),\n",
    "    'model': best_en,\n",
    "    'use_scaled': True\n",
    "}\n",
    "\n",
    "# Print model comparison\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"{model_name}: {results['mean_rmse']:.2f} ± {results['std_rmse']:.2f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['mean_rmse'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "print(f\"\\nBest Model: {best_model_name} (RMSE: {model_results[best_model_name]['mean_rmse']:.2f})\")\n",
    "\n",
    "# ========================================\n",
    "# 10. ENSEMBLE MODEL\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n=== Creating Ensemble Model ===\")\n",
    "\n",
    "# Create ensemble predictions using top 3 models\n",
    "sorted_models = sorted(model_results.items(), key=lambda x: x[1]['mean_rmse'])\n",
    "top_3_models = sorted_models[:3]\n",
    "\n",
    "print(\"Top 3 models for ensemble:\")\n",
    "for name, results in top_3_models:\n",
    "    print(f\"- {name}: {results['mean_rmse']:.2f}\")\n",
    "\n",
    "def ensemble_predict(models_info, X_data, X_data_scaled, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1/len(models_info)] * len(models_info)\n",
    "    \n",
    "    predictions = []\n",
    "    for i, (name, info) in enumerate(models_info):\n",
    "        model = info['model']\n",
    "        use_scaled = info.get('use_scaled', False)\n",
    "        \n",
    "        if use_scaled:\n",
    "            pred = model.predict(X_data_scaled)\n",
    "        else:\n",
    "            pred = model.predict(X_data)\n",
    "        \n",
    "        predictions.append(pred * weights[i])\n",
    "    \n",
    "    return np.sum(predictions, axis=0)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_scores = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    X_train_scaled_fold, X_val_scaled_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train models on fold\n",
    "    fold_models_info = []\n",
    "    for name, info in top_3_models:\n",
    "        model = info['model']\n",
    "        model_copy = type(model)(**model.get_params())\n",
    "        \n",
    "        if info.get('use_scaled', False):\n",
    "            model_copy.fit(X_train_scaled_fold, y_train_fold)\n",
    "        else:\n",
    "            model_copy.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        fold_models_info.append((name, {'model': model_copy, 'use_scaled': info.get('use_scaled', False)}))\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = ensemble_predict(fold_models_info, X_val_fold, X_val_scaled_fold)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "    ensemble_scores.append(rmse)\n",
    "    print(f\"Ensemble RMSE: {rmse:.2f}\")\n",
    "\n",
    "ensemble_mean_rmse = np.mean(ensemble_scores)\n",
    "print(f\"Ensemble Mean RMSE: {ensemble_mean_rmse:.2f} ± {np.std(ensemble_scores):.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 11. FINAL PREDICTIONS AND SUBMISSION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n=== Final Training and Predictions ===\")\n",
    "\n",
    "# Train final models on full dataset\n",
    "final_models_info = []\n",
    "for name, info in top_3_models:\n",
    "    model = info['model']\n",
    "    final_model = type(model)(**model.get_params())\n",
    "    \n",
    "    if info.get('use_scaled', False):\n",
    "        final_model.fit(X_scaled, y)\n",
    "    else:\n",
    "        final_model.fit(X, y)\n",
    "    \n",
    "    final_models_info.append((name, {'model': final_model, 'use_scaled': info.get('use_scaled', False)}))\n",
    "\n",
    "# Make final predictions\n",
    "if ensemble_mean_rmse < model_results[best_model_name]['mean_rmse']:\n",
    "    print(\"Using ensemble model for final predictions\")\n",
    "    final_predictions = ensemble_predict(final_models_info, X_test, X_test_scaled)\n",
    "else:\n",
    "    print(f\"Using {best_model_name} for final predictions\")\n",
    "    best_model.fit(X, y) if not model_results[best_model_name].get('use_scaled', False) else best_model.fit(X_scaled, y)\n",
    "    \n",
    "    if model_results[best_model_name].get('use_scaled', False):\n",
    "        final_predictions = best_model.predict(X_test_scaled)\n",
    "    else:\n",
    "        final_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "final_predictions = np.maximum(final_predictions, 0)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'route_key': df_test['route_key'],\n",
    "    'final_seatcount': final_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission_file.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission file created with {len(submission)} predictions\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"Mean: {final_predictions.mean():.2f}\")\n",
    "print(f\"Std: {final_predictions.std():.2f}\")\n",
    "print(f\"Min: {final_predictions.min():.2f}\")\n",
    "print(f\"Max: {final_predictions.max():.2f}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n=== Feature Importance (from best tree-based model) ===\")\n",
    "tree_based_models = ['Random Forest', 'Extra Trees', 'Gradient Boosting']\n",
    "best_tree_model = None\n",
    "for name in tree_based_models:\n",
    "    if name == best_model_name:\n",
    "        best_tree_model = best_model\n",
    "        break\n",
    "\n",
    "if best_tree_model is not None:\n",
    "    importance = best_tree_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(15))\n",
    "else:\n",
    "    print(\"Best model is not tree-based, skipping feature importance\")\n",
    "\n",
    "print(\"\\n=== Solution Complete ===\")\n",
    "print(\"Files created:\")\n",
    "print(\"- submission_file.csv: Final predictions for submission\")\n",
    "print(f\"- Best model: {best_model_name}\")\n",
    "print(f\"- Expected RMSE: {model_results[best_model_name]['mean_rmse']:.2f}\")\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(model_results.keys())\n",
    "rmse_means = [model_results[name]['mean_rmse'] for name in model_names]\n",
    "rmse_stds = [model_results[name]['std_rmse'] for name in model_names]\n",
    "\n",
    "plt.errorbar(model_names, rmse_means, yerr=rmse_stds, fmt='o', capsize=5, capthick=2)\n",
    "plt.title('Model Performance Comparison (RMSE)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel comparison plot saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07d688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0c07f7b",
   "metadata": {},
   "source": [
    "# gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data files loaded successfully.\n",
      "\n",
      "--- Starting Data Cleaning and Preprocessing ---\n",
      "Date columns converted to datetime objects.\n",
      "\n",
      "--- Engineering Features ---\n",
      "Feature engineering complete.\n",
      "\n",
      "--- Preparing Model Training and Test Sets ---\n",
      "Filtered transactions for dbd = 15. Shape: (73100, 18)\n",
      "Final training data shape after merge: (67200, 19)\n",
      "Final test data shape after merge: (5900, 19)\n",
      "\n",
      "--- Training XGBoost Model ---\n",
      "Model training complete.\n",
      "\n",
      "--- Generating Predictions and Submission File ---\n",
      "Submission file 'submission_file.csv' created successfully.\n",
      "\n",
      "Top 5 rows of the submission file:\n",
      "          route_key  final_seatcount\n",
      "0  2025-02-11_46_45             3732\n",
      "1  2025-01-20_17_23             1629\n",
      "2  2025-01-08_02_14             1171\n",
      "3  2025-01-08_08_47              962\n",
      "4  2025-01-08_09_46             3305\n",
      "\n",
      "--- Top 15 Feature Importances ---\n",
      "        Value      Feature\n",
      "114  0.035489   route_46_9\n",
      "135  0.030618   route_9_46\n",
      "144  0.029741      srcid_9\n",
      "188  0.027359     destid_9\n",
      "99   0.027158  route_45_46\n",
      "177  0.026984     srcid_45\n",
      "169  0.026830     srcid_37\n",
      "111  0.025360  route_46_45\n",
      "225  0.024738    destid_48\n",
      "215  0.023375    destid_37\n",
      "121  0.023267  route_47_45\n",
      "113  0.022572  route_46_48\n",
      "100  0.020462  route_45_47\n",
      "79   0.020332  route_36_37\n",
      "82   0.019073  route_37_36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "# Updated file paths based on your new code structure.\n",
    "try:\n",
    "    df_train = pd.read_csv('training_data/train/train.csv')\n",
    "    df_transactions = pd.read_csv('training_data/train/transactions.csv')\n",
    "    df_test = pd.read_csv('testing data/test_8gqdJqH.csv')\n",
    "    print(\"All data files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    print(\"Please ensure your folder structure and file names are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Initial Data Cleaning & Type Conversion ---\n",
    "print(\"\\n--- Starting Data Cleaning and Preprocessing ---\")\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "for df in [df_train, df_transactions, df_test]:\n",
    "    df['doj'] = pd.to_datetime(df['doj'])\n",
    "if 'doi' in df_transactions.columns:\n",
    "    df_transactions['doi'] = pd.to_datetime(df_transactions['doi'])\n",
    "\n",
    "print(\"Date columns converted to datetime objects.\")\n",
    "\n",
    "# --- 3. Feature Engineering Function ---\n",
    "# We create a function to apply the same transformations to both train and test data\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Creates time-series and categorical features from the dataframe.\n",
    "    \"\"\"\n",
    "    df['month'] = df['doj'].dt.month\n",
    "    df['year'] = df['doj'].dt.year\n",
    "    df['day_of_week'] = df['doj'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['day_of_year'] = df['doj'].dt.dayofyear\n",
    "    df['week_of_year'] = df['doj'].dt.isocalendar().week.astype(int)\n",
    "    df['is_weekend'] = (df['doj'].dt.dayofweek >= 5).astype(int) # Saturday or Sunday\n",
    "    \n",
    "    # Create a unique route identifier\n",
    "    df['route'] = df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n--- Engineering Features ---\")\n",
    "# Apply full feature engineering to the transactions data\n",
    "df_transactions = create_features(df_transactions)\n",
    "\n",
    "# For the test set, we only need to create the 'route' column for the merge key.\n",
    "# The other features (month, year, etc.) will come from the transaction data after the merge.\n",
    "df_test['route'] = df_test['srcid'].astype(str) + '_' + df_test['destid'].astype(str)\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# --- 4. Prepare Training and Test Data ---\n",
    "print(\"\\n--- Preparing Model Training and Test Sets ---\")\n",
    "\n",
    "# Filter transactions for data available exactly 15 days before departure\n",
    "dbd_filter = 15\n",
    "df_transactions_filtered = df_transactions[df_transactions['dbd'] == dbd_filter].copy()\n",
    "\n",
    "print(f\"Filtered transactions for dbd = {dbd_filter}. Shape: {df_transactions_filtered.shape}\")\n",
    "\n",
    "# Create the training set by merging filtered transactions with train labels\n",
    "df_model_train = pd.merge(\n",
    "    df_train,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Final training data shape after merge: {df_model_train.shape}\")\n",
    "if df_model_train.shape[0] != df_train.shape[0]:\n",
    "    print(\"Warning: Some routes in train.csv did not have a transaction record at dbd=15.\")\n",
    "\n",
    "# Create the test set by merging filtered transactions with test routes\n",
    "df_model_test = pd.merge(\n",
    "    df_test,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid', 'route'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Final test data shape after merge: {df_model_test.shape}\")\n",
    "if df_model_test.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: Some test routes have missing features. Filling with 0.\")\n",
    "    df_model_test.fillna(0, inplace=True)\n",
    "\n",
    "# --- 5. Model Training (XGBoost) ---\n",
    "print(\"\\n--- Training XGBoost Model ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# We will one-hot encode the categorical features for XGBoost\n",
    "features = [\n",
    "    'srcid', 'destid', 'cumsum_seatcount', 'cumsum_searchcount',\n",
    "    'month', 'year', 'day_of_week', 'day_of_year', 'week_of_year', 'is_weekend',\n",
    "    'srcid_region', 'destid_region', 'srcid_tier', 'destid_tier', 'route'\n",
    "]\n",
    "categorical_features = ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier', 'route', 'srcid', 'destid']\n",
    "\n",
    "X = df_model_train[features]\n",
    "y_train = df_model_train['final_seatcount']\n",
    "X_test_prep = df_model_test[features]\n",
    "\n",
    "# One-Hot Encode categorical features\n",
    "X_train = pd.get_dummies(X, columns=categorical_features, dummy_na=False)\n",
    "X_test = pd.get_dummies(X_test_prep, columns=categorical_features, dummy_na=False)\n",
    "\n",
    "# Align columns - crucial for ensuring test set has same features as train set\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# XGBoost Model Parameters\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist', # Use 'hist' for faster training\n",
    "    'early_stopping_rounds': 100 # Moved parameter here\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "# Train the model with early stopping\n",
    "# The early_stopping_rounds parameter from the constructor will be used here.\n",
    "model.fit(X_train, y_train,\n",
    "          eval_set=[(X_train, y_train)],\n",
    "          verbose=False)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "train_predictions = model.predict(X_train)\n",
    "train_rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "print(f\"\\nFinal Training RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- Generating Predictions and Submission File ---\")\n",
    "\n",
    "# Predict on the test data\n",
    "# The model automatically uses the best iteration thanks to early stopping\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Post-processing: Ensure predictions are non-negative integers\n",
    "predictions[predictions < 0] = 0\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Create the submission file\n",
    "submission_df = pd.DataFrame({'route_key': df_model_test['route_key'], 'final_seatcount': predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission_file.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_file.csv' created successfully.\")\n",
    "print(\"\\nTop 5 rows of the submission file:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# --- 7. (Optional) Feature Importance ---\n",
    "print(\"\\n--- Top 15 Feature Importances ---\")\n",
    "feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': X_train.columns})\n",
    "print(feature_imp.sort_values(by=\"Value\", ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b069ff8",
   "metadata": {},
   "source": [
    "# LSTM Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data files loaded successfully.\n",
      "\n",
      "--- Starting Data Cleaning and Preprocessing ---\n",
      "Date columns converted to datetime objects.\n",
      "\n",
      "--- Engineering Advanced Time Series Features ---\n",
      "Advanced time series feature engineering complete.\n",
      "\n",
      "--- Preparing Time Series Data ---\n",
      "Filtered transactions for dbd = 15. Shape: (73100, 74)\n",
      "Training data shape after merge: (67200, 75)\n",
      "\n",
      "--- Encoding Categorical Variables ---\n",
      "\n",
      "--- Creating LSTM Sequences ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df_train = pd.read_csv('training_data/train/train.csv')\n",
    "    df_transactions = pd.read_csv('training_data/train/transactions.csv')\n",
    "    df_test = pd.read_csv('testing data/test_8gqdJqH.csv')\n",
    "    print(\"All data files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    print(\"Please ensure your folder structure and file names are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Initial Data Cleaning & Type Conversion ---\n",
    "print(\"\\n--- Starting Data Cleaning and Preprocessing ---\")\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "for df in [df_train, df_transactions, df_test]:\n",
    "    df['doj'] = pd.to_datetime(df['doj'])\n",
    "if 'doi' in df_transactions.columns:\n",
    "    df_transactions['doi'] = pd.to_datetime(df_transactions['doi'])\n",
    "\n",
    "print(\"Date columns converted to datetime objects.\")\n",
    "\n",
    "# --- 3. Advanced Time Series Feature Engineering ---\n",
    "def create_time_series_features(df):\n",
    "    \"\"\"\n",
    "    Creates comprehensive time-series features optimized for LSTM models.\n",
    "    \"\"\"\n",
    "    # Basic time features\n",
    "    df['month'] = df['doj'].dt.month\n",
    "    df['year'] = df['doj'].dt.year\n",
    "    df['day_of_week'] = df['doj'].dt.dayofweek\n",
    "    df['day_of_year'] = df['doj'].dt.dayofyear\n",
    "    df['week_of_year'] = df['doj'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['doj'].dt.quarter\n",
    "    df['is_weekend'] = (df['doj'].dt.dayofweek >= 5).astype(int)\n",
    "    df['is_monday'] = (df['doj'].dt.dayofweek == 0).astype(int)\n",
    "    df['is_friday'] = (df['doj'].dt.dayofweek == 4).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for better time representation\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    # Holiday indicators (approximate)\n",
    "    df['is_holiday_season'] = ((df['month'] == 12) | (df['month'] == 1)).astype(int)\n",
    "    df['is_summer'] = ((df['month'] >= 5) & (df['month'] <= 7)).astype(int)\n",
    "    df['is_winter'] = ((df['month'] >= 11) | (df['month'] <= 2)).astype(int)\n",
    "    \n",
    "    # Create unique route identifier\n",
    "    df['route'] = df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col, lags=[1, 2, 3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Creates lag features for time series analysis.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df.groupby('route')[target_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col, windows=[3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Creates rolling statistics features.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df[f'{target_col}_rolling_mean_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling std\n",
    "        df[f'{target_col}_rolling_std_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling max\n",
    "        df[f'{target_col}_rolling_max_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).max().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling min\n",
    "        df[f'{target_col}_rolling_min_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).min().reset_index(0, drop=True)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n--- Engineering Advanced Time Series Features ---\")\n",
    "# Apply feature engineering\n",
    "df_transactions = create_time_series_features(df_transactions)\n",
    "df_test = create_time_series_features(df_test)\n",
    "\n",
    "# Add lag and rolling features to transactions\n",
    "df_transactions = create_lag_features(df_transactions, 'cumsum_seatcount')\n",
    "df_transactions = create_lag_features(df_transactions, 'cumsum_searchcount')\n",
    "df_transactions = create_rolling_features(df_transactions, 'cumsum_seatcount')\n",
    "df_transactions = create_rolling_features(df_transactions, 'cumsum_searchcount')\n",
    "\n",
    "print(\"Advanced time series feature engineering complete.\")\n",
    "\n",
    "# --- 4. Prepare Time Series Data ---\n",
    "print(\"\\n--- Preparing Time Series Data ---\")\n",
    "\n",
    "# Filter transactions for dbd = 15\n",
    "dbd_filter = 15\n",
    "df_transactions_filtered = df_transactions[df_transactions['dbd'] == dbd_filter].copy()\n",
    "\n",
    "print(f\"Filtered transactions for dbd = {dbd_filter}. Shape: {df_transactions_filtered.shape}\")\n",
    "\n",
    "# Create training dataset\n",
    "df_model_train = pd.merge(\n",
    "    df_train,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Training data shape after merge: {df_model_train.shape}\")\n",
    "\n",
    "# Fill missing values for lag and rolling features\n",
    "lag_roll_cols = [col for col in df_model_train.columns if ('lag_' in col or 'rolling_' in col)]\n",
    "df_model_train[lag_roll_cols] = df_model_train[lag_roll_cols].fillna(0)\n",
    "\n",
    "# --- 5. Encode Categorical Variables ---\n",
    "print(\"\\n--- Encoding Categorical Variables ---\")\n",
    "\n",
    "# Label encode categorical variables\n",
    "categorical_cols = ['srcid', 'destid', 'srcid_region', 'destid_region', 'srcid_tier', 'destid_tier', 'route']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model_train[f'{col}_encoded'] = le.fit_transform(df_model_train[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- 6. Create Sequences for LSTM ---\n",
    "def create_sequences(data, target_col, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Creates sequences for LSTM training.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Sort by route and date\n",
    "    data_sorted = data.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by route to create sequences\n",
    "    for route in data_sorted['route'].unique():\n",
    "        route_data = data_sorted[data_sorted['route'] == route].copy()\n",
    "        \n",
    "        if len(route_data) >= sequence_length:\n",
    "            for i in range(len(route_data) - sequence_length + 1):\n",
    "                sequence = route_data.iloc[i:i+sequence_length]\n",
    "                target = route_data.iloc[i+sequence_length-1][target_col]\n",
    "                \n",
    "                # Select features for sequence\n",
    "                feature_cols = [\n",
    "                    'cumsum_seatcount', 'cumsum_searchcount',\n",
    "                    'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "                    'day_of_year_sin', 'day_of_year_cos', 'quarter',\n",
    "                    'is_weekend', 'is_monday', 'is_friday', 'is_holiday_season',\n",
    "                    'is_summer', 'is_winter',\n",
    "                    'srcid_encoded', 'destid_encoded'\n",
    "                ] + lag_roll_cols\n",
    "                \n",
    "                # Only include columns that exist in the data\n",
    "                available_cols = [col for col in feature_cols if col in sequence.columns]\n",
    "                sequence_features = sequence[available_cols].values\n",
    "                \n",
    "                sequences.append(sequence_features)\n",
    "                targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "print(\"\\n--- Creating LSTM Sequences ---\")\n",
    "sequence_length = 10  # Reduced sequence length for better training\n",
    "X_sequences, y_sequences = create_sequences(df_model_train, 'final_seatcount', sequence_length)\n",
    "\n",
    "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
    "\n",
    "# --- 7. Scale Features ---\n",
    "print(\"\\n--- Scaling Features ---\")\n",
    "\n",
    "# Reshape for scaling\n",
    "n_samples, n_timesteps, n_features = X_sequences.shape\n",
    "X_reshaped = X_sequences.reshape(-1, n_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# Scale target\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y_sequences.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"Feature scaling complete. Final X shape: {X_scaled.shape}\")\n",
    "\n",
    "# --- 8. Split Data ---\n",
    "print(\"\\n--- Splitting Data for Training ---\")\n",
    "\n",
    "# Use time-based split (80-20)\n",
    "split_idx = int(0.8 * len(X_scaled))\n",
    "X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_val = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "\n",
    "# --- 9. Build LSTM Model ---\n",
    "print(\"\\n--- Building LSTM Model ---\")\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Creates an optimized LSTM model for time series forecasting.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_lstm_model((sequence_length, n_features))\n",
    "print(model.summary())\n",
    "\n",
    "# --- 10. Train Model ---\n",
    "print(\"\\n--- Training LSTM Model ---\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=8,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 11. Evaluate Model ---\n",
    "print(\"\\n--- Evaluating Model ---\")\n",
    "\n",
    "# Make predictions\n",
    "train_pred_scaled = model.predict(X_train)\n",
    "val_pred_scaled = model.predict(X_val)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_pred = target_scaler.inverse_transform(train_pred_scaled)\n",
    "val_pred = target_scaler.inverse_transform(val_pred_scaled)\n",
    "y_train_orig = target_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_val_orig = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_orig, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred))\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# --- 12. Prepare Test Data and Make Predictions ---\n",
    "print(\"\\n--- Preparing Test Data for Prediction ---\")\n",
    "\n",
    "# For test predictions, we need to create sequences similar to training\n",
    "# First, encode test categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in df_test.columns:\n",
    "        # Handle unseen categories\n",
    "        df_test[f'{col}_encoded'] = df_test[col].astype(str).map(\n",
    "            dict(zip(label_encoders[col].classes_, range(len(label_encoders[col].classes_))))\n",
    "        ).fillna(0).astype(int)\n",
    "\n",
    "# Create test dataset\n",
    "df_model_test = pd.merge(\n",
    "    df_test,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid', 'route'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "df_model_test = df_model_test.fillna(0)\n",
    "\n",
    "# For test predictions, we'll use the last sequence from training data for each route\n",
    "# and predict the next value\n",
    "print(\"\\n--- Generating Test Predictions ---\")\n",
    "\n",
    "test_predictions = []\n",
    "route_keys = []\n",
    "\n",
    "for _, test_row in df_model_test.iterrows():\n",
    "    route = test_row['route']\n",
    "    route_key = test_row['route_key']\n",
    "    \n",
    "    # Find historical data for this route\n",
    "    route_history = df_model_train[df_model_train['route'] == route].copy()\n",
    "    \n",
    "    if len(route_history) >= sequence_length:\n",
    "        # Use the last sequence_length records\n",
    "        route_history = route_history.sort_values('doj').tail(sequence_length)\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = [\n",
    "            'cumsum_seatcount', 'cumsum_searchcount',\n",
    "            'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "            'day_of_year_sin', 'day_of_year_cos', 'quarter',\n",
    "            'is_weekend', 'is_monday', 'is_friday', 'is_holiday_season',\n",
    "            'is_summer', 'is_winter',\n",
    "            'srcid_encoded', 'destid_encoded'\n",
    "        ] + lag_roll_cols\n",
    "        \n",
    "        available_cols = [col for col in feature_cols if col in route_history.columns]\n",
    "        sequence = route_history[available_cols].values\n",
    "        \n",
    "        # Reshape and scale\n",
    "        sequence = sequence.reshape(1, sequence_length, len(available_cols))\n",
    "        sequence_scaled = scaler.transform(sequence.reshape(-1, len(available_cols)))\n",
    "        sequence_scaled = sequence_scaled.reshape(1, sequence_length, len(available_cols))\n",
    "        \n",
    "        # Predict\n",
    "        pred_scaled = model.predict(sequence_scaled, verbose=0)\n",
    "        pred = target_scaler.inverse_transform(pred_scaled)[0, 0]\n",
    "        \n",
    "        # Ensure non-negative\n",
    "        pred = max(0, pred)\n",
    "        test_predictions.append(int(round(pred)))\n",
    "    else:\n",
    "        # If no sufficient history, use mean of available data or 0\n",
    "        if len(route_history) > 0:\n",
    "            pred = route_history['final_seatcount'].mean()\n",
    "        else:\n",
    "            pred = 0\n",
    "        test_predictions.append(int(round(pred)))\n",
    "    \n",
    "    route_keys.append(route_key)\n",
    "\n",
    "# --- 13. Create Submission File ---\n",
    "print(\"\\n--- Creating Submission File ---\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'route_key': route_keys,\n",
    "    'final_seatcount': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('lstm_submission_file.csv', index=False)\n",
    "\n",
    "print(\"LSTM submission file 'lstm_submission_file.csv' created successfully.\")\n",
    "print(f\"\\nFinal Validation RMSE: {val_rmse:.4f}\")\n",
    "print(\"\\nTop 5 rows of the submission file:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"Mean prediction: {np.mean(test_predictions):.2f}\")\n",
    "print(f\"Std prediction: {np.std(test_predictions):.2f}\")\n",
    "print(f\"Min prediction: {np.min(test_predictions)}\")\n",
    "print(f\"Max prediction: {np.max(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f5e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec054031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
