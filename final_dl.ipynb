{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744cbdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/work/hackathon_redus/redbus_hackathon/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data files loaded successfully.\n",
      "\n",
      "--- Starting Data Cleaning and Preprocessing ---\n",
      "Date columns converted to datetime objects.\n",
      "\n",
      "--- Engineering Advanced Time Series Features ---\n",
      "Advanced time series feature engineering complete.\n",
      "\n",
      "--- Preparing Time Series Data ---\n",
      "Filtered transactions for dbd = 15. Shape: (73100, 74)\n",
      "Training data shape after merge: (67200, 75)\n",
      "\n",
      "--- Encoding Categorical Variables ---\n",
      "\n",
      "--- Creating LSTM Sequences ---\n",
      "Created 66300 sequences with shape (66300, 10, 61)\n",
      "\n",
      "--- Scaling Features ---\n",
      "Feature scaling complete. Final X shape: (66300, 10, 61)\n",
      "\n",
      "--- Splitting Data for Training ---\n",
      "Training set: (53040, 10, 61), Validation set: (13260, 10, 61)\n",
      "\n",
      "--- Building LSTM Model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">97,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m97,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,225</span> (641.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m164,225\u001b[0m (641.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">163,777</span> (639.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m163,777\u001b[0m (639.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "--- Training LSTM Model ---\n",
      "Epoch 1/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.5558 - mae: 0.5252 - val_loss: 0.2956 - val_mae: 0.3601 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2266 - mae: 0.3354 - val_loss: 0.2039 - val_mae: 0.2951 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.1910 - mae: 0.3054 - val_loss: 0.1460 - val_mae: 0.2729 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1817 - mae: 0.2943 - val_loss: 0.1757 - val_mae: 0.2914 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1667 - mae: 0.2832 - val_loss: 0.1764 - val_mae: 0.2872 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1554 - mae: 0.2755 - val_loss: 0.1601 - val_mae: 0.2640 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.1490 - mae: 0.2713 - val_loss: 0.1802 - val_mae: 0.2778 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1392 - mae: 0.2630 - val_loss: 0.1775 - val_mae: 0.2767 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1354 - mae: 0.2591 - val_loss: 0.1520 - val_mae: 0.2662 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1344 - mae: 0.2587 - val_loss: 0.1545 - val_mae: 0.2644 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m1652/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1270 - mae: 0.2514\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1270 - mae: 0.2514 - val_loss: 0.1615 - val_mae: 0.2700 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1195 - mae: 0.2450 - val_loss: 0.1267 - val_mae: 0.2469 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1109 - mae: 0.2383 - val_loss: 0.1292 - val_mae: 0.2505 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1065 - mae: 0.2341 - val_loss: 0.1255 - val_mae: 0.2455 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1070 - mae: 0.2338 - val_loss: 0.1312 - val_mae: 0.2522 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 364ms/step - loss: 0.1029 - mae: 0.2298 - val_loss: 0.1192 - val_mae: 0.2423 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1004 - mae: 0.2276 - val_loss: 0.1288 - val_mae: 0.2487 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0997 - mae: 0.2269 - val_loss: 0.1212 - val_mae: 0.2456 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0973 - mae: 0.2256 - val_loss: 0.1255 - val_mae: 0.2476 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0980 - mae: 0.2256 - val_loss: 0.1297 - val_mae: 0.2470 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0940 - mae: 0.2200 - val_loss: 0.1273 - val_mae: 0.2479 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0943 - mae: 0.2208 - val_loss: 0.1298 - val_mae: 0.2526 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0917 - mae: 0.2190 - val_loss: 0.1311 - val_mae: 0.2500 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m1652/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0933 - mae: 0.2203\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0933 - mae: 0.2203 - val_loss: 0.1340 - val_mae: 0.2576 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0883 - mae: 0.2147 - val_loss: 0.1226 - val_mae: 0.2462 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0828 - mae: 0.2093 - val_loss: 0.1294 - val_mae: 0.2492 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0841 - mae: 0.2101 - val_loss: 0.1325 - val_mae: 0.2511 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0835 - mae: 0.2093 - val_loss: 0.1268 - val_mae: 0.2485 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0814 - mae: 0.2085 - val_loss: 0.1307 - val_mae: 0.2509 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0827 - mae: 0.2077 - val_loss: 0.1280 - val_mae: 0.2499 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0812 - mae: 0.2070 - val_loss: 0.1361 - val_mae: 0.2538 - learning_rate: 2.5000e-04\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "--- Evaluating Model ---\n",
      "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Training RMSE: 302.3385\n",
      "Validation RMSE: 413.3742\n",
      "\n",
      "--- Preparing Test Data for Prediction ---\n",
      "\n",
      "--- Generating Test Predictions ---\n",
      "\n",
      "--- Creating Submission File ---\n",
      "LSTM submission file 'lstm_submission_file.csv' created successfully.\n",
      "\n",
      "Final Validation RMSE: 413.3742\n",
      "\n",
      "Top 5 rows of the submission file:\n",
      "          route_key  final_seatcount\n",
      "0  2025-02-11_46_45             3761\n",
      "1  2025-01-20_17_23             1895\n",
      "2  2025-01-08_02_14             1363\n",
      "3  2025-01-08_08_47             1356\n",
      "4  2025-01-08_09_46             5636\n",
      "\n",
      "Prediction statistics:\n",
      "Mean prediction: 2265.45\n",
      "Std prediction: 982.48\n",
      "Min prediction: 197\n",
      "Max prediction: 5966\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df_train = pd.read_csv('training_data/train/train.csv')\n",
    "    df_transactions = pd.read_csv('training_data/train/transactions.csv')\n",
    "    df_test = pd.read_csv('testing data/test_8gqdJqH.csv')\n",
    "    print(\"All data files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    print(\"Please ensure your folder structure and file names are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Initial Data Cleaning & Type Conversion ---\n",
    "print(\"\\n--- Starting Data Cleaning and Preprocessing ---\")\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "for df in [df_train, df_transactions, df_test]:\n",
    "    df['doj'] = pd.to_datetime(df['doj'])\n",
    "if 'doi' in df_transactions.columns:\n",
    "    df_transactions['doi'] = pd.to_datetime(df_transactions['doi'])\n",
    "\n",
    "print(\"Date columns converted to datetime objects.\")\n",
    "\n",
    "# --- 3. Advanced Time Series Feature Engineering ---\n",
    "def create_time_series_features(df):\n",
    "    \"\"\"\n",
    "    Creates comprehensive time-series features optimized for LSTM models.\n",
    "    \"\"\"\n",
    "    # Basic time features\n",
    "    df['month'] = df['doj'].dt.month\n",
    "    df['year'] = df['doj'].dt.year\n",
    "    df['day_of_week'] = df['doj'].dt.dayofweek\n",
    "    df['day_of_year'] = df['doj'].dt.dayofyear\n",
    "    df['week_of_year'] = df['doj'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['doj'].dt.quarter\n",
    "    df['is_weekend'] = (df['doj'].dt.dayofweek >= 5).astype(int)\n",
    "    df['is_monday'] = (df['doj'].dt.dayofweek == 0).astype(int)\n",
    "    df['is_friday'] = (df['doj'].dt.dayofweek == 4).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for better time representation\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    # Holiday indicators (approximate)\n",
    "    df['is_holiday_season'] = ((df['month'] == 12) | (df['month'] == 1)).astype(int)\n",
    "    df['is_summer'] = ((df['month'] >= 5) & (df['month'] <= 7)).astype(int)\n",
    "    df['is_winter'] = ((df['month'] >= 11) | (df['month'] <= 2)).astype(int)\n",
    "    \n",
    "    # Create unique route identifier\n",
    "    df['route'] = df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col, lags=[1, 2, 3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Creates lag features for time series analysis.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df.groupby('route')[target_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col, windows=[3, 7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Creates rolling statistics features.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df[f'{target_col}_rolling_mean_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling std\n",
    "        df[f'{target_col}_rolling_std_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling max\n",
    "        df[f'{target_col}_rolling_max_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).max().reset_index(0, drop=True)\n",
    "        )\n",
    "        # Rolling min\n",
    "        df[f'{target_col}_rolling_min_{window}'] = (\n",
    "            df.groupby('route')[target_col].rolling(window=window, min_periods=1).min().reset_index(0, drop=True)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n--- Engineering Advanced Time Series Features ---\")\n",
    "# Apply feature engineering\n",
    "df_transactions = create_time_series_features(df_transactions)\n",
    "df_test = create_time_series_features(df_test)\n",
    "\n",
    "# Add lag and rolling features to transactions\n",
    "df_transactions = create_lag_features(df_transactions, 'cumsum_seatcount')\n",
    "df_transactions = create_lag_features(df_transactions, 'cumsum_searchcount')\n",
    "df_transactions = create_rolling_features(df_transactions, 'cumsum_seatcount')\n",
    "df_transactions = create_rolling_features(df_transactions, 'cumsum_searchcount')\n",
    "\n",
    "print(\"Advanced time series feature engineering complete.\")\n",
    "\n",
    "# --- 4. Prepare Time Series Data ---\n",
    "print(\"\\n--- Preparing Time Series Data ---\")\n",
    "\n",
    "# Filter transactions for dbd = 15\n",
    "dbd_filter = 15\n",
    "df_transactions_filtered = df_transactions[df_transactions['dbd'] == dbd_filter].copy()\n",
    "\n",
    "print(f\"Filtered transactions for dbd = {dbd_filter}. Shape: {df_transactions_filtered.shape}\")\n",
    "\n",
    "# Create training dataset\n",
    "df_model_train = pd.merge(\n",
    "    df_train,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Training data shape after merge: {df_model_train.shape}\")\n",
    "\n",
    "# Fill missing values for lag and rolling features\n",
    "lag_roll_cols = [col for col in df_model_train.columns if ('lag_' in col or 'rolling_' in col)]\n",
    "df_model_train[lag_roll_cols] = df_model_train[lag_roll_cols].fillna(0)\n",
    "\n",
    "# --- 5. Encode Categorical Variables ---\n",
    "print(\"\\n--- Encoding Categorical Variables ---\")\n",
    "\n",
    "# Label encode categorical variables\n",
    "categorical_cols = ['srcid', 'destid', 'srcid_region', 'destid_region', 'srcid_tier', 'destid_tier', 'route']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model_train[f'{col}_encoded'] = le.fit_transform(df_model_train[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- 6. Create Sequences for LSTM ---\n",
    "def create_sequences(data, target_col, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Creates sequences for LSTM training.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Sort by route and date\n",
    "    data_sorted = data.sort_values(['route', 'doj']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by route to create sequences\n",
    "    for route in data_sorted['route'].unique():\n",
    "        route_data = data_sorted[data_sorted['route'] == route].copy()\n",
    "        \n",
    "        if len(route_data) >= sequence_length:\n",
    "            for i in range(len(route_data) - sequence_length + 1):\n",
    "                sequence = route_data.iloc[i:i+sequence_length]\n",
    "                target = route_data.iloc[i+sequence_length-1][target_col]\n",
    "                \n",
    "                # Select features for sequence\n",
    "                feature_cols = [\n",
    "                    'cumsum_seatcount', 'cumsum_searchcount',\n",
    "                    'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "                    'day_of_year_sin', 'day_of_year_cos', 'quarter',\n",
    "                    'is_weekend', 'is_monday', 'is_friday', 'is_holiday_season',\n",
    "                    'is_summer', 'is_winter',\n",
    "                    'srcid_encoded', 'destid_encoded'\n",
    "                ] + lag_roll_cols\n",
    "                \n",
    "                # Only include columns that exist in the data\n",
    "                available_cols = [col for col in feature_cols if col in sequence.columns]\n",
    "                sequence_features = sequence[available_cols].values\n",
    "                \n",
    "                sequences.append(sequence_features)\n",
    "                targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "print(\"\\n--- Creating LSTM Sequences ---\")\n",
    "sequence_length = 10  # Reduced sequence length for better training\n",
    "X_sequences, y_sequences = create_sequences(df_model_train, 'final_seatcount', sequence_length)\n",
    "\n",
    "print(f\"Created {len(X_sequences)} sequences with shape {X_sequences.shape}\")\n",
    "\n",
    "# --- 7. Scale Features ---\n",
    "print(\"\\n--- Scaling Features ---\")\n",
    "\n",
    "# Reshape for scaling\n",
    "n_samples, n_timesteps, n_features = X_sequences.shape\n",
    "X_reshaped = X_sequences.reshape(-1, n_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# Scale target\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y_sequences.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"Feature scaling complete. Final X shape: {X_scaled.shape}\")\n",
    "\n",
    "# --- 8. Split Data ---\n",
    "print(\"\\n--- Splitting Data for Training ---\")\n",
    "\n",
    "# Use time-based split (80-20)\n",
    "split_idx = int(0.8 * len(X_scaled))\n",
    "X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_val = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "\n",
    "# --- 9. Build LSTM Model ---\n",
    "print(\"\\n--- Building LSTM Model ---\")\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Creates an optimized LSTM model for time series forecasting.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_lstm_model((sequence_length, n_features))\n",
    "print(model.summary())\n",
    "\n",
    "# --- 10. Train Model ---\n",
    "print(\"\\n--- Training LSTM Model ---\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=8,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 11. Evaluate Model ---\n",
    "print(\"\\n--- Evaluating Model ---\")\n",
    "\n",
    "# Make predictions\n",
    "train_pred_scaled = model.predict(X_train)\n",
    "val_pred_scaled = model.predict(X_val)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_pred = target_scaler.inverse_transform(train_pred_scaled)\n",
    "val_pred = target_scaler.inverse_transform(val_pred_scaled)\n",
    "y_train_orig = target_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_val_orig = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_orig, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_orig, val_pred))\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# --- 12. Prepare Test Data and Make Predictions ---\n",
    "print(\"\\n--- Preparing Test Data for Prediction ---\")\n",
    "\n",
    "# For test predictions, we need to create sequences similar to training\n",
    "# First, encode test categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in df_test.columns:\n",
    "        # Handle unseen categories\n",
    "        df_test[f'{col}_encoded'] = df_test[col].astype(str).map(\n",
    "            dict(zip(label_encoders[col].classes_, range(len(label_encoders[col].classes_))))\n",
    "        ).fillna(0).astype(int)\n",
    "\n",
    "# Create test dataset\n",
    "df_model_test = pd.merge(\n",
    "    df_test,\n",
    "    df_transactions_filtered,\n",
    "    on=['doj', 'srcid', 'destid', 'route'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values\n",
    "df_model_test = df_model_test.fillna(0)\n",
    "\n",
    "# For test predictions, we'll use the last sequence from training data for each route\n",
    "# and predict the next value\n",
    "print(\"\\n--- Generating Test Predictions ---\")\n",
    "\n",
    "test_predictions = []\n",
    "route_keys = []\n",
    "\n",
    "for _, test_row in df_model_test.iterrows():\n",
    "    route = test_row['route']\n",
    "    route_key = test_row['route_key']\n",
    "    \n",
    "    # Find historical data for this route\n",
    "    route_history = df_model_train[df_model_train['route'] == route].copy()\n",
    "    \n",
    "    if len(route_history) >= sequence_length:\n",
    "        # Use the last sequence_length records\n",
    "        route_history = route_history.sort_values('doj').tail(sequence_length)\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = [\n",
    "            'cumsum_seatcount', 'cumsum_searchcount',\n",
    "            'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "            'day_of_year_sin', 'day_of_year_cos', 'quarter',\n",
    "            'is_weekend', 'is_monday', 'is_friday', 'is_holiday_season',\n",
    "            'is_summer', 'is_winter',\n",
    "            'srcid_encoded', 'destid_encoded'\n",
    "        ] + lag_roll_cols\n",
    "        \n",
    "        available_cols = [col for col in feature_cols if col in route_history.columns]\n",
    "        sequence = route_history[available_cols].values\n",
    "        \n",
    "        # Reshape and scale\n",
    "        sequence = sequence.reshape(1, sequence_length, len(available_cols))\n",
    "        sequence_scaled = scaler.transform(sequence.reshape(-1, len(available_cols)))\n",
    "        sequence_scaled = sequence_scaled.reshape(1, sequence_length, len(available_cols))\n",
    "        \n",
    "        # Predict\n",
    "        pred_scaled = model.predict(sequence_scaled, verbose=0)\n",
    "        pred = target_scaler.inverse_transform(pred_scaled)[0, 0]\n",
    "        \n",
    "        # Ensure non-negative\n",
    "        pred = max(0, pred)\n",
    "        test_predictions.append(int(round(pred)))\n",
    "    else:\n",
    "        # If no sufficient history, use mean of available data or 0\n",
    "        if len(route_history) > 0:\n",
    "            pred = route_history['final_seatcount'].mean()\n",
    "        else:\n",
    "            pred = 0\n",
    "        test_predictions.append(int(round(pred)))\n",
    "    \n",
    "    route_keys.append(route_key)\n",
    "\n",
    "# --- 13. Create Submission File ---\n",
    "print(\"\\n--- Creating Submission File ---\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'route_key': route_keys,\n",
    "    'final_seatcount': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('lstm_submission_file.csv', index=False)\n",
    "\n",
    "print(\"LSTM submission file 'lstm_submission_file.csv' created successfully.\")\n",
    "print(f\"\\nFinal Validation RMSE: {val_rmse:.4f}\")\n",
    "print(\"\\nTop 5 rows of the submission file:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"Mean prediction: {np.mean(test_predictions):.2f}\")\n",
    "print(f\"Std prediction: {np.std(test_predictions):.2f}\")\n",
    "print(f\"Min prediction: {np.min(test_predictions)}\")\n",
    "print(f\"Max prediction: {np.max(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa5fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
